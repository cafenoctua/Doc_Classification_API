# Ref
- [bert-japanese](https://github.com/yoheikikuta/bert-japanese)
- [BERT with SentencePiece で日本語専用の pre-trained モデルを学習し、それを基にタスクを解く](https://techlife.cookpad.com/entry/2018/12/04/093000)
- [第3回 BERT を用いた自然言語処理における転移学習](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part3.html)
- [BERTを用いて、日本語文章の多値分類を行う](https://qiita.com/Yuu94/items/0e5cff226bd3cc8fb08c)
- [Activation Functions Explained - GELU, SELU, ELU, ReLU and more](https://mlfromscratch.com/activation-functions-explained/#/)
- [Googleが公開しているBERT（自然言語処理AI）のファイル解説](https://qiita.com/uedake722/items/63a1ea32fe84886c02f9)
- [BERTを動かしてKaggle過去コンペ（Quora Question Pairs）にSubmit](https://qiita.com/t-kosugi/items/4481b77bab748b8ebdca)
- [PYTORCHでBERTの日本語学習済みモデルを利用する - 文章埋め込み編](https://yag-ays.github.io/project/pytorch_bert_japanese/)
- [[翻訳]BERTで自然言語AIをはじめる（github上のREADMEの翻訳）](https://qiita.com/uedake722/items/927bf491a025f1a88b17#using-bert-in-colabcolab%E4%B8%8A%E3%81%A7bert%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B)
- [Beginner’s Guide to BERT for Multi-classification Task](https://towardsdatascience.com/beginners-guide-to-bert-for-multi-classification-task-92f5445c2d7c)
## 学習済みモデル
- [BERT日本語Pretrainedモデル](http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB)
- [Laboro.AIオリジナル日本語版BERTモデルを公開](https://laboro.ai/column/laboro-bert/)←基本的には非商用利用